# Optimización con SGD vs el algoritmo ADAM

## Integrantes:

Sebastián Sabat

Rocío Yáñez

## Tema principal:

Algoritmo de gradiente estocástico/Deep Learning

## Resumen:

El objetivo de esta investigación es entregar un predic
tor del rendimiento de estudiantes en la prueba específi
ca de matemáticas. Para ello, se trabajó con una base de
datos que contenía los resultados en todas las pruebas y
características sociodemográficas de los estudiantes de la
promoción. Mas particularmente, se comparo el algoritmo de
SGD (sthocastic gradient descent) con ADAM. Este ultimo
consiste principalmente en dos componentes: descenso de 
gradiente de minibatch y programas de tasa de aprendizaje

## Referencias:

[1] Mehmood, F.; Ahmad, S.; Whangbo, T.K. An Ef-
ficient Optimization Technique for Training Deep
Neural Networks. Mathematics 2023, 11, 1360.
https://doi.org/10.3390/math11061360

[2] Bottou, L. (1998). On-line learning and stochastic
approximations. In On-Line Learning in Neural Net-
works, 9–42. Ver 
https://wiki.eecs.yorku.ca/course_archive/2012-13/F/6328/_media/bottou-onlinelearning-98.pdf

[3] Centro de estudios, Mineduc, Datos abiertos. PAES
2023, Prueba de Acceso a la Educación 2023 - Inscri-
tos Puntajes. Disponible https://datosabiertos.mineduc.cl/pruebas-de-admision-a-la-educacion-superior/
