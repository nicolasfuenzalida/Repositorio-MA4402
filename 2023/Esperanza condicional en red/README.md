# Implementaci√≥n num√©rica de esperanzas condicionales usando redes neuronales

## Integrantes:

Melanie S√°nchez Pfeier

## Tema principal:

Algoritmos estoc√°sticos en aprendizaje de m√°quinas: Algoritmo de gradiente estoc√°stico (SGD)

## Resumen:

Dadas X, Y variables aleatorias reales en
(Œ©, F, P), se sabe que la esperanza condicional 
E(Y |X) es la proyecci√≥n ortogonal de Y en
(Œ©, œÉ(X), P), por lo tanto, es la funci√≥n medi-
ble de X m√°s cercana a Y en L2. Es decir,
f = E(Y |X) es la √∫nica soluci√≥n del problema
min E(f(X)-Y)^2 s.a f‚ààK
donde K es el conjunto de todas las funciones
medibles f : R ‚Üí R tal que f (X) ‚àà L2(Œ©, F, P).
Por el Teorema de Hornik sobre la universalidad
de redes neuronales en L2(Œº) se propone entrenar una red neuronal
que aproxime la solucion del problema.

## Referencias:

[1] Kurt Hornik, Maxwell Stinchcombe, and
Halber White. MultilayerFeedforward Networksare Universal Approximators. Neural
Networks, Vol2, pp.359-366,1989.

[2] Phillipp Grohs, and Gitta Kutyniok. Mathematical aspects of deep learning. Cambridge University Press, 2023.

[3] Apuntes Curso MA5606-1: T√≥picos Matem√°ticos en Aprendizaje de M√°quinas, Redes Neuronales y Aprendizaje Profundo.
Profesores Joaqu√≠n Fontbona y Claudio
Mu√±oz, 2023.
